---
title: "X causes Y"
author: "Macartan"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{X-causes-Y}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(gbiqq)
library(dplyr)
```

We will walk through a simple model in which $X$ causes $Y$. We will assume uninformative priors ("Jeffreys priors").

# Define the model

```{r}
model    <- make_model("X -> Y") %>%
	
	          set_priors(prior_distribution = "jeffreys")
```

# Graph it

```{r}
plot_dag(model)
```

# View parameter matrix

```{r}
get_parameter_matrix(model)
```

Note that in a simple  "$X$ causes $Y$" model there are four "nodal types" for $Y$---ways that $Y$ can respond to $X$. These correspond to the last four rows of the parameter matrix. Here $Yij$ means that Y takes on value $i$ when $X=0$ and $j$ when $X=1$. There are just two possible types for $X$.  In all there are eight causal types -- given by the columns of the parameter matrix.

# Draw data

We can draw data directly from the model, in which case a parameter vector is draw from the priors and used to generate data. Alternatively we can imagine a "true" parameter vector and use that to simulate data. Here is one in which the parameter vector---corresponding to the rows of the parameter matrix --- imply  that $X=1$ with probability .5, that $X$ has a positive effect on $Y$ for half the units, and that $X$ never has a negative effect on $Y$. 

```{r}
data <- simulate_data(model, n = 5000, lambda = c(.5, .5, .25, .5, 0, .25))
table(data)
```

# Calculate posteriors

```{r, message = FALSE, warning = FALSE, results = "hide"}
updated_model      <- gbiqq(model, data, refresh = 0)
```

You can look at the `stan` output like this:

```{r}
updated_model$posterior_distribution
```

# Calculate estimands

We first calculate a posterior distribution for the  average causal effect of $X$ on $Y$. This quantity is identified and posterior looks like this. 


```{r} 
estimand <- estimand_distribution(
                   model = updated_model, 
                   posterior = TRUE,
                   query = "Y[X=1] - Y[X=0]"
                   )

hist(estimand, xlim = c(0,1))  
```


Now lets ask about  the probability that $X=1$ caused $Y=1$ for those cases in which $X=Y=1$. The right answer is 0.67 -- that is, given the assumed true data generating process, in two thirds of cases with $X=Y=1$, $Y=1$ because $X=1$. This is the "probabilty of causation", PC. However, this quantity is not identified by $X,Y$ data alone. All that we can be sure of is that PC lies between .67 and 1 (see for example Dawid 2015). 

How do our inferences look given the uninformative prior?  

```{r} 
prior_estimand <- estimand_distribution(
                   model = updated_model, 
                   posterior = FALSE,
                   query = "Y[X=1] > Y[X=0]",
                   subset = "X==1 & Y==1"
                   )
posterior_estimand <- estimand_distribution(
                   model = updated_model, 
                   posterior = TRUE,
                   query = "Y[X=1] > Y[X=0]",
                   subset = "X==1 & Y==1"
                   )
par(mfrow = c(1,2))
hist(prior_estimand, xlim = c(0,1), main = "Prior on Probability of Causation")  
hist(posterior_estimand, xlim = c(0,1), main = "Posterior on Probability of Causation")  
```


We find that they do not converge but they do place positive mass in the right range. Within this range, the shape of the posterior depends on the priors only. 
